{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "acrobot_DQN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "odNaDE1zyrL2"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanderdavies/acrobot/blob/main/acrobot_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab Rendering"
      ]
    },
    {
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[classic_control] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from itertools import count\n",
        "import random"
      ],
      "metadata": {
        "id": "1mo4JanfqV67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelObs(gym.Wrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    self.observations = deque([], maxlen=2)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.env.reset()\n",
        "    self.observations.clear()\n",
        "    self.observations.append(self._get_screen()) # previous observation\n",
        "    return self._get_ob()\n",
        "\n",
        "  def step(self, action):\n",
        "    ob, reward, done, info = self.env.step(action)\n",
        "    return self._get_ob(), reward, done, info\n",
        "\n",
        "  def _get_screen(self):\n",
        "    screen = torch.from_numpy(np.ascontiguousarray(env.render(mode=\"rgb_array\")))\n",
        "    return screen.permute(2, 0, 1).float()\n",
        "\n",
        "  def _get_ob(self):\n",
        "    self.observations.append(self._get_screen())\n",
        "    return self.observations[1] - self.observations[0]"
      ],
      "metadata": {
        "id": "g5IfX-YcpXlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make(\"Acrobot-v1\"))\n",
        "env = PixelObs(env)\n",
        "\n",
        "# env.reset();\n",
        "# observation, reward, done, info = env.step(0) "
      ],
      "metadata": {
        "id": "J3IajB2MyCOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "BcRrqvShvXUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, 5, 2)\n",
        "    self.batch1 = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 5, 2)\n",
        "    self.batch2 = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 32, 5, 2)\n",
        "    self.batch3 = nn.BatchNorm2d(32)\n",
        "    self.head = nn.Linear(32*59*59, 3)\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.batch1(self.conv1(x)))\n",
        "    x = F.relu(self.batch2(self.conv2(x)))\n",
        "    x = F.relu(self.batch3(self.conv3(x)))\n",
        "    return self.head(x.view(x.size(0), -1))"
      ],
      "metadata": {
        "id": "K_HN0fUHtNA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, capacity):\n",
        "    self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "  def append(self, transition):\n",
        "    self.memory.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "4WfWOWbi34gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "cell_type": "code",
      "source": [
        "# get environment\n",
        "env = wrap_env(gym.make(\"Acrobot-v1\"))\n",
        "env = PixelObs(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "cell_type": "code",
      "source": [
        "# show env preview with random actions \n",
        "sample_replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "obs = env.reset()\n",
        "while True:\n",
        "    \n",
        "    action = env.action_space.sample() \n",
        "    \n",
        "    new_obs, reward, done, info = env.step(action) \n",
        "    if done:\n",
        "      new_obs = None\n",
        "    sample_replay_buffer.append((obs, action, reward, new_obs))\n",
        "    obs = new_obs\n",
        "\n",
        "    env.render()\n",
        "    \n",
        "    if done: \n",
        "      break;     \n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPS = 10 # Over 300\n",
        "EPS_THRESH = 0.01\n",
        "BATCH_SIZE = 64\n",
        "TARGET_UPDATE = 10 # Number of episodes before updating Target network\n",
        "GAMMA = 0.999\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"using {DEVICE}\")"
      ],
      "metadata": {
        "id": "SHz9pmiN6EJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init(project=\"acrobot\", tags=[f\"bs_{batch_size}\", f\"eps_tresh_{EPS_THRESH}\"])\n",
        "# get dqns\n",
        "policy_model = DQN().to(DEVICE)\n",
        "target_model = DQN().to(DEVICE)\n",
        "target_model.load_state_dict(policy_model.state_dict());"
      ],
      "metadata": {
        "id": "GCDXg72Lt-YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.RMSprop(policy_model.parameters())\n",
        "\n",
        "def optimize_model(policy_model, target_model, replay_buffer):\n",
        "  if len(replay_buffer) < BATCH_SIZE:\n",
        "    return\n",
        "\n",
        "  batch = replay_buffer.sample(BATCH_SIZE)\n",
        "  state, action, reward, new_state = zip(*batch)\n",
        "  reward_batch = torch.tensor(reward).unsqueeze(1).to(DEVICE)\n",
        "  state_batch = torch.stack(state).to(DEVICE)\n",
        "  action_batch = torch.tensor(action).unsqueeze(1).to(DEVICE)\n",
        "\n",
        "  # q\n",
        "  q = policy_model(state_batch).gather(1, action_batch)\n",
        "\n",
        "  # non final states\n",
        "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, new_state))).to(DEVICE) \n",
        "  non_final_new_states = torch.stack([s for s in new_state if s is not None]).to(DEVICE)\n",
        "\n",
        "  # updated q\n",
        "  v = target_model(non_final_new_states).max(1)[0].unsqueeze(1).to(DEVICE)\n",
        "  new_q = reward_batch\n",
        "  new_q[non_final_mask] += GAMMA * v\n",
        "\n",
        "  loss = criterion(q, new_q)  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  for param in policy_model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "  optimizer.step()\n",
        "  wandb.log({\"Loss\": loss})"
      ],
      "metadata": {
        "id": "LDma3Zt2z0nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "replay_buffer = ReplayBuffer(10000)\n",
        "episode_lengths = []\n",
        "\n",
        "for ep in tqdm(range(NUM_EPS)):\n",
        "  obs = env.reset()\n",
        "\n",
        "  for length in count():\n",
        "    sample = np.random.rand()\n",
        "    \n",
        "    if sample > EPS_THRESH:\n",
        "      action = torch.argmax(policy_model(obs.unsqueeze(0).to(DEVICE))[0])\n",
        "    \n",
        "    else:\n",
        "      action = env.action_space.sample() \n",
        "    \n",
        "    new_obs, reward, done, info = env.step(action) \n",
        "    if done:\n",
        "      new_obs = None\n",
        "    replay_buffer.append((obs, action, reward, new_obs))\n",
        "    obs = new_obs\n",
        "\n",
        "    optimize_model(policy_model, target_model, replay_buffer)\n",
        "  \n",
        "    if done:\n",
        "      episode_lengths.append(length+1)\n",
        "      wandb.log({\"Episode\": ep, \"Episode Length\": length+1})\n",
        "      break\n",
        "  if ep % TARGET_UPDATE == 0:\n",
        "    target_model.load_state_dict(policy_model.state_dict())\n",
        "  print(f\"Last episode length {episode_lengths[-1]}\")\n"
      ],
      "metadata": {
        "id": "0f2S0zFIv6LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show durations over time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(episode_lengths)\n",
        "plt.ylabel(\"Episode Lengths\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "mXXjdcWDKkmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HHUi39n888ex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}